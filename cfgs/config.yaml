defaults:
  - _self_
  - task@_global_: ???
  - learner@_global_: ddpg

# standard params
project_name: ???
seed: 0
level: med_exp
train_byol: false

# environment parameters
img_size: 64
frame_stack: 3
action_repeat: 2

# world model training + RND model training
obs_shape: ???
action_shape: ???
model_train_epochs: 1000
model_batch_size: 50
seq_len: 10
max_steps: 1000000 # to be on par with D4RL

# general args (world model + policies + everything else)
feature_dim: 200
hidden_dim: 128
dreamer: true
depth: 32

# dataset args
num_trajs: 200
normalize_inputs: true
normalize_state_only: true
sample_batches: true
torch_data: false
use_fn_dataset: false

# reward augmentations
aug: rnd
clip: false

# what to train
load_model: false

# bc warmstart args
bc_warmstart: false
bc_epochs: 100

# policy training args
policy_train_epochs: 1000
policy_rb_capacity: 1000000
policy_batch_size: 50

# policy evaluation args
policy_eval_every: 100
num_eval_episodes: 10
policy_save_every: 100

# logging
wandb: true
save_model: true
eval_model: true
model_save_every: 100
model_eval_every: 100

# agent training params (common for all)
reward_min: -1.0
reward_max: 1.0
penalize: true
penalize_q: false

# parallelization
pmap: false

# BYOL-Explore + Dreamer world model config
byol:
  task: ${task}
  seed: ${seed}
  obs_shape: ${obs_shape}
  action_shape: ${action_shape}
  optim: adam
  lr: 3e-4
  ema: 0.99
  pmap: ${pmap}

  # dreamer args
  stoch_dim: 32
  stoch_discrete_dim: 32
  beta: 1.0

  vd4rl:
    obs_shape: ${obs_shape}
    seq_len: ${seq_len}
    dreamer: ${dreamer}
    use_ln: true
    depth: ${depth}
    gru_hidden_size: 200
    hidden_dim: 200
    stoch_dim: ${byol.stoch_dim}
    stoch_discrete_dim: ${byol.stoch_discrete_dim}
  
  d4rl:
    obs_shape: ${obs_shape}
    seq_len: ${seq_len}
    hidden_dim: 512
    repr_dim: 128
    gru_hidden_size: 64
    stoch_discrete_dim: ${byol.stoch_discrete_dim}

# RND model config
rnd:
  task: ${task}
  seed: ${seed}
  obs_shape: ${obs_shape}
  action_shape: ${action_shape}
  optim: adam
  lr: 1e-3
  cat_actions: false
  pmap: ${pmap}
  l1: false

  vd4rl:
    dreamer: ${dreamer}
    depth: ${depth}
    hidden_dim: 256
    repr_dim: 50
  
  d4rl:
    hidden_dim: 256
    repr_dim: 50

# Autoencoder config
ae:
  task: ${task}
  seed: ${seed}
  hidden_dim: 256
  obs_shape: ${obs_shape}
  action_shape: ${action_shape}
  type: cond_ae
  
  feature_dim: 200
  state_embed_dim: 16
  action_embed_dim: 16
  beta: 0.5
  clip_log_std: true
  activate_final: true
  lr: 1e-3

simple_dynamics:
  seed: ${seed}
  model_type: mlp_dynamics
  obs_shape: ${obs_shape}
  action_shape: ${action_shape}
  hidden_dim: ${hidden_dim}
  optim: sgd
  lr: 1e-4
  train_for_diff: true
  transform: ${transform}

hydra:
  run:
    dir: ./local_runs/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}