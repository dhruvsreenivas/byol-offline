defaults:
  - _self_
  - task@_global_: ???
  - learner@_global_: ???

project_name: ???
seed: 0
level: med_exp
train_byol: true

# environment parameters
img_size: 64
frame_stack: 1
action_repeat: 2

# world model training + RND model training
obs_shape: ???
action_shape: ???
model_train_epochs: 5000
model_batch_size: 50
seq_len: 50
max_steps: 10000

feature_dim: 200
hidden_dim: 128

dreamer: true
depth: 32

# reward augmentations
reward_aug: rnd

# what to train
load_model: false

# policy training args
policy_train_steps: 30000000
num_initial_rand_steps: 2500
policy_rb_capacity: 20000
policy_batch_size: 256

# policy evaluation args
policy_eval_every: 10000
num_eval_episodes: 10
policy_save_every: 1000

# logging
wandb: true
save_model: true
model_save_every: 100

byol:
  task: ${task}
  vd4rl:
    obs_shape: ${obs_shape}
    action_shape: ${action_shape}
    seq_len: ${seq_len}
    dreamer: ${dreamer}
    depth: ${depth}

    gru_hidden_size: 256
    seed: ${seed}
    lr: 1e-3
    ema: 0.99
  
  d4rl:
    obs_shape: ${obs_shape}
    action_shape: ${action_shape}
    seq_len: ${seq_len}
    hidden_dim: 512
    repr_dim: 128

    gru_hidden_size: 64
    seed: ${seed}
    lr: 1e-3
    ema: 0.99

rnd:
  task: ${task}
  vd4rl:
    obs_shape: ${obs_shape}
    dreamer: ${dreamer}
    depth: ${depth}
    seed: ${seed}

    hidden_dim: ${hidden_dim}
    repr_dim: 50
    lr: 1e-3

hydra:
  run:
    dir: ./local_runs/${now:%Y.%m.%d}/${now:%H%M%S}_${hydra.job.override_dirname}